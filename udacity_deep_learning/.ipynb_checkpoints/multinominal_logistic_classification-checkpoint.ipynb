{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多项分布 逻辑斯蒂分类 (softmax + 交叉熵 cross entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 输入x是一个向量；一个三分类问题，则输出y是一个三个元素向量；\n",
    "\n",
    "2. 逻辑斯蒂分类是线性分类器， 即 y=Wx+b, W是矩阵，x, y, b都是向量；\n",
    "\n",
    "3. 线性分类器的输出需要转为一个概率值，即通过softmax函数； softmax函数公式见最下方；\n",
    "\n",
    "4. 一个训练样本有一个对应的标签，这个标签用1-hot labels来表示；通过softmax的输出概率和1-hot labels的交叉熵（距离D）来判断（验证、训练,）；交叉熵的公式在最下面。\n",
    "\n",
    "###注意 \n",
    "这个多项分布 逻辑斯蒂分类和之前的逻辑斯蒂回归不一样，这个使用了softmax概率输出以及交叉熵， 逻辑斯蒂回归用了sigmoid函数作为概率输出；\n",
    "\n",
    "这两个的区别：\n",
    "\n",
    "A probabilistic interpretation is that the softmax leads you to model the joint distribution over the output variables <em>p(x1, x2, x3, ..., xn) </em>whereas using sigmoids leads you to model the marginal distributions <em>p(x1), p(x2), p(x3), ..., p(xn)</em>.\n",
    "Maybe a more hands on way of thinking about it is that pushing up the value for output i will lead the probability of other outputs to go down if you're using a softmax. For a sigmoid, it won't affect the probabilities for the other outputs.\n",
    "Perhaps an even more hands on way to think about it is that sigmoids make sense when there are lots of independent yes/no labels. For example, if you have a classifier that predicts male vs. female, young vs. old, etc. Whereas the softmax makes sense for exclusive classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是寻找W和b，让训练误差最小；\n",
    "\n",
    "训练误差，就是让正确的标签交叉熵小，让错误的标签交叉熵大；\n",
    "\n",
    "误差是整体误差，在整个训练集上。\n",
    "\n",
    "优化W和b，用梯度下降。\n",
    "\n",
    "![](images/5.PNG)\n",
    "![](images/6.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般采用随机梯度下降或者小batch的随机梯度下降。\n",
    "\n",
    "调整参数每次更新的大小比较重要，一般要加入动量，动量就是每次都记住上一步的更新值，加到这一步去。\n",
    "\n",
    "![](images/8.PNG)\n",
    "\n",
    "## SGD的替代\n",
    "还有一个叫AdaGrad,省去了调整动量，学习速率的麻烦。\n",
    "\n",
    "学习速率可以用模拟退火，让它不断减小。\n",
    "\n",
    "![](images/7.PNG)\n",
    "![](images/9.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
